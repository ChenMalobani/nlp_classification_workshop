{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os, sys, re, collections, string\n",
    "from operator import itemgetter as at\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact\n",
    "sys.path.append(\"../python\")\n",
    "import data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "X,y = data.preprocessed()\n",
    "sample_doc = X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a vocabulary by distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44277/44277 [04:06<00:00, 179.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 147462785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_count = collections.Counter()\n",
    "for x in tqdm(X):\n",
    "    word_count += collections.Counter(x.split())\n",
    "total_word_count = sum(word_count.values())\n",
    "word_count_hist = collections.Counter(word_count.values())\n",
    "print (\"Total word count: \"+str(total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315e73e6b4ed454188b2cdb4d63c00d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='lb', max=10000, min=1), FloatSlider(value=10000.0, description='ub', max=10000000.0, min=1000.0), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lb = (1,10000), ub =(1000,1e7))\n",
    "def vocab_coverage(lb=10,ub=10000):\n",
    "    words_covered = sum([wc*n for wc, n in word_count_hist.items() if lb<wc<ub])\n",
    "    corpus_percentage = words_covered/total_word_count\n",
    "    vector_size = len([1 for wc, n in word_count_hist.items() if lb<wc<ub])\n",
    "    return \"Corpus Coverage: {c:.2f}%\\n Vector Size: {v}\".format(c=corpus_percentage*100,v=vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency Vector\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.05, max_features=None, min_df=1e-06,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "cv = text.CountVectorizer(min_df=1e-06, max_df=0.05)\n",
    "cv.fit(X[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vec = cv.transform([sample_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector to bag of words\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abingdon': 1,\n",
       " 'accessing': 1,\n",
       " 'acknowledgements': 1,\n",
       " 'administratively': 1,\n",
       " 'advantage': 1,\n",
       " 'allow': 1,\n",
       " 'alpha': 22,\n",
       " 'alphas': 4,\n",
       " 'asset': 1,\n",
       " 'barney': 1,\n",
       " 'box': 1,\n",
       " 'cdrom': 1,\n",
       " 'competitors': 1,\n",
       " 'comprise': 1,\n",
       " 'contacts': 1,\n",
       " 'contested': 1,\n",
       " 'critical': 1,\n",
       " 'employmentchange': 1,\n",
       " 'exception': 1,\n",
       " 'expedited': 1,\n",
       " 'familiar': 1,\n",
       " 'feasible': 1,\n",
       " 'growth': 1,\n",
       " 'highly': 1,\n",
       " 'importance': 1,\n",
       " 'inactions': 1,\n",
       " 'incentives': 1,\n",
       " 'internet': 2,\n",
       " 'intranet': 1,\n",
       " 'latest': 1,\n",
       " 'lawsuits': 1,\n",
       " 'levels': 1,\n",
       " 'link': 1,\n",
       " 'maintaining': 1,\n",
       " 'measures': 1,\n",
       " 'messenger': 1,\n",
       " 'misuse': 1,\n",
       " 'natural': 2,\n",
       " 'necessarily': 2,\n",
       " 'nominees': 1,\n",
       " 'oppose': 1,\n",
       " 'paper': 2,\n",
       " 'performancebased': 1,\n",
       " 'permanently': 1,\n",
       " 'po': 1,\n",
       " 'pricing': 1,\n",
       " 'procurement': 1,\n",
       " 'protective': 1,\n",
       " 'ratio': 1,\n",
       " 'recipients': 15,\n",
       " 'recognizes': 1,\n",
       " 'safeguard': 1,\n",
       " 'send': 1,\n",
       " 'significantly': 1,\n",
       " 'site': 2,\n",
       " 'sixtieth': 1,\n",
       " 'smith': 1,\n",
       " 'strategic': 1,\n",
       " 'strategies': 1,\n",
       " 'targets': 1,\n",
       " 'telecopy': 1,\n",
       " 'timing': 1,\n",
       " 'unearned': 1,\n",
       " 'uses': 1,\n",
       " 'va': 1,\n",
       " 'vary': 1,\n",
       " 'virginia': 1,\n",
       " 'web': 1,\n",
       " 'wide': 1,\n",
       " 'wwwbenefitaccesscom': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in zip(cv.get_feature_names(), sample_vec.toarray()[0]) if v>0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'factor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"factorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abingdon': 1,\n",
       " 'advantag': 1,\n",
       " 'alpha': 26,\n",
       " 'barney': 1,\n",
       " 'box': 1,\n",
       " 'cdrom': 1,\n",
       " 'critic': 1,\n",
       " 'employmentchang': 1,\n",
       " 'expedit': 1,\n",
       " 'familiar': 1,\n",
       " 'feasibl': 1,\n",
       " 'growth': 1,\n",
       " 'highli': 1,\n",
       " 'import': 1,\n",
       " 'internet': 2,\n",
       " 'intranet': 1,\n",
       " 'latest': 1,\n",
       " 'lawsuit': 1,\n",
       " 'link': 1,\n",
       " 'messeng': 1,\n",
       " 'misus': 1,\n",
       " 'necessarili': 2,\n",
       " 'oppos': 1,\n",
       " 'performancebas': 1,\n",
       " 'po': 1,\n",
       " 'procur': 1,\n",
       " 'ratio': 1,\n",
       " 'safeguard': 1,\n",
       " 'significantli': 1,\n",
       " 'site': 2,\n",
       " 'sixtieth': 1,\n",
       " 'smith': 1,\n",
       " 'strateg': 1,\n",
       " 'unearn': 1,\n",
       " 'va': 1,\n",
       " 'vari': 1,\n",
       " 'virginia': 1,\n",
       " 'web': 1,\n",
       " 'wide': 1,\n",
       " 'wwwbenefitaccesscom': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = text.CountVectorizer().build_analyzer()\n",
    "def my_analyzer(txt):\n",
    "    return [stemmer.stem(w) for w in analyzer(txt)]\n",
    "cv = text.CountVectorizer(min_df=1e-06, max_df=0.05, analyzer=my_analyzer)\n",
    "cv.fit(X[:100])\n",
    "sample_vec = cv.transform([sample_doc])\n",
    "{k:v for k,v in zip(cv.get_feature_names(), sample_vec.toarray()[0]) if v>0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency inverse document frequency (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abiom': 0.03698348952694716,\n",
       " 'actiga': 0.03698348952694716,\n",
       " 'aemploye': 0.03698348952694716,\n",
       " 'albanna': 0.031775212329687975,\n",
       " 'allot': 0.028728565475930265,\n",
       " 'attorneysinfact': 0.028728565475930265,\n",
       " 'australia': 0.8136367695928376,\n",
       " 'autom': 0.14793395810778864,\n",
       " 'bryan': 0.028728565475930265,\n",
       " 'cheryl': 0.03393684267318945,\n",
       " 'constat': 0.03698348952694716,\n",
       " 'depositari': 0.03698348952694716,\n",
       " 'eighth': 0.028728565475930265,\n",
       " 'embarrass': 0.03698348952694716,\n",
       " 'exemplari': 0.03393684267318945,\n",
       " 'exofficio': 0.03393684267318945,\n",
       " 'forefeit': 0.03698348952694716,\n",
       " 'lift': 0.03698348952694716,\n",
       " 'miner': 0.03393684267318945,\n",
       " 'motiv': 0.03393684267318945,\n",
       " 'nonacceler': 0.030098521573929036,\n",
       " 'nondisparag': 0.03698348952694716,\n",
       " 'plaza': 0.031775212329687975,\n",
       " 'procur': 0.030098521573929036,\n",
       " 'reinsur': 0.03698348952694716,\n",
       " 'rendit': 0.03698348952694716,\n",
       " 'repeatedli': 0.028728565475930265,\n",
       " 'shalhoub': 0.06019704314785807,\n",
       " 'shortterm': 0.03698348952694716,\n",
       " 'sureti': 0.03393684267318945,\n",
       " 'sustain': 0.03698348952694716,\n",
       " 'tend': 0.031775212329687975,\n",
       " 'thereat': 0.03698348952694716,\n",
       " 'transmitt': 0.028728565475930265,\n",
       " 'unexercis': 0.031775212329687975,\n",
       " 'upward': 0.03393684267318945,\n",
       " 'viedgar': 0.03393684267318945,\n",
       " 'whic': 0.06019704314785807,\n",
       " 'wide': 0.06355042465937595,\n",
       " 'xinshun': 0.028728565475930265}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tfidf = text.TfidfVectorizer(min_df=1e-06, max_df=0.05)\n",
    "tfidf.fit(X[:100])\n",
    "sample_vec = tfidf.transform([sample_doc])\n",
    "{k:v for k,v in zip(cv.get_feature_names(), sample_vec.toarray()[0]) if v>0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Pretrained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec.load(\"../data/w2v.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('supplier', 0.8100154399871826),\n",
       " ('identities', 0.7488305568695068),\n",
       " ('vendor', 0.7211676836013794),\n",
       " ('customers', 0.719849705696106),\n",
       " ('client', 0.7191833853721619),\n",
       " ('wholesaler', 0.6979198455810547),\n",
       " ('retailer', 0.6949663162231445),\n",
       " ('referrer', 0.6916903853416443),\n",
       " ('subscriber', 0.6891459226608276),\n",
       " ('advertisers', 0.6872985363006592)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7971811 ,  1.7461873 , -1.7333413 , -0.9501938 ,  1.4369504 ,\n",
       "       -4.573448  , -1.040323  ,  1.8452846 , -6.584528  ,  5.045666  ,\n",
       "        2.3666062 , -2.4841917 , -1.2176125 , -5.4686084 , -1.5029901 ,\n",
       "       -3.9456666 , -1.1921363 , -2.3597367 , -1.5567018 , -0.01446523,\n",
       "        0.2594563 , -2.0609753 , -6.4029737 ,  1.6582427 , -3.0667682 ,\n",
       "       -0.5999931 ,  0.2741809 ,  2.3116817 ,  1.7087582 , -5.688676  ,\n",
       "       -3.125402  ,  2.4790525 , -1.5706098 ,  2.2393646 ,  1.1512741 ,\n",
       "       -0.86816007,  1.7273517 ,  0.5174984 ,  3.1057346 ,  0.765412  ,\n",
       "        4.303741  , -1.8025136 ,  3.2532973 ,  0.74829334,  0.51785934,\n",
       "        0.21034734,  0.9509934 , -7.1567492 , -3.2851288 , -3.233999  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv[\"customer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging word vectors across the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7507401 , -0.420542  , -0.10012057, -0.37326655,  0.5822965 ,\n",
       "        0.77466947,  0.6441445 ,  0.3625388 ,  0.43520534, -0.54652923,\n",
       "       -0.28740865, -1.3981526 , -0.412218  ,  0.32011726,  0.09712528,\n",
       "        0.22358079, -0.75756776, -0.70142686, -0.21124128,  0.10119429,\n",
       "       -0.8234234 ,  0.4335564 ,  0.67138153,  0.5240189 , -0.92881304,\n",
       "        0.528634  ,  1.1133226 , -0.54446703, -0.12792982,  0.18315491,\n",
       "       -0.5741413 ,  0.3953018 , -0.3179198 , -0.5529575 , -1.0513256 ,\n",
       "        1.1475902 , -0.19089158,  0.1173111 ,  0.53352356, -1.6372716 ,\n",
       "       -0.02028674, -0.2380265 ,  0.20073438,  0.55285066,  0.9066735 ,\n",
       "       -0.6627023 , -0.7060955 , -0.14574422, -0.63928986, -0.58009225],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = None\n",
    "for w in nltk.word_tokenize(sample_doc):\n",
    "    try:\n",
    "        if v is not None:\n",
    "            v+=w2v.wv[w]\n",
    "            n+=1\n",
    "        else:\n",
    "            v=np.copy(w2v.wv[w])\n",
    "            n=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "v/=n\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word vectors averaging could be combined with TF/IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
